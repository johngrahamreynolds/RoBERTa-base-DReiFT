{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8723bc7a-ac60-4a3b-8fd8-769288f79979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and Preprocess our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1ae907-7ea6-4f2c-8de0-ce7bd9ffda22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load our slightly preprocessed dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "cleaned = load_dataset(\"MarioBarbeque/UCI_drug_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664dfb54-5078-4831-91b5-7c87e9d0ffce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define our labels as the set of all 805 possible medical conditions across our dataset\n",
    "\n",
    "from datasets import ClassLabel\n",
    "\n",
    "train_conditions = set(cleaned[\"train\"].unique(\"condition\"))\n",
    "validate_conditions = set(cleaned[\"validation\"].unique(\"condition\"))\n",
    "test_conditions = set(cleaned[\"test\"].unique(\"condition\"))\n",
    "conditions = train_conditions | validate_conditions | test_conditions # union operator\n",
    "condition_labels = ClassLabel(num_classes=len(conditions), names=list(conditions))\n",
    "\n",
    "# Mapping Labels to IDs\n",
    "def map_label_to_class_index(example):\n",
    "    example['label'] = condition_labels.str2int(example['condition'])\n",
    "    return example\n",
    "\n",
    "def map_class_index_to_label(example):\n",
    "    example['label_name'] = condition_labels.int2str(example['label'])\n",
    "    return example\n",
    "\n",
    "labeled = cleaned.map(map_label_to_class_index, batched=True)\n",
    "labeled = labeled.map(map_class_index_to_label, batched=True)\n",
    "\n",
    "condition_labels.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8444c815-bfbd-4cde-ae5d-5d39231d3af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define the tokenzier and data collator, as inhereted from RoBERTa\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"review\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = cleaned.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdc0c6a-5830-4b1a-a00f-0e4b976c1c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "137249a7-84b0-4e5f-a10d-b3241cc419af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove all extraneous columns\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"patient_id\", \"drugName\", \"condition\", \"review\", \"rating\", \"date\", \"usefulCount\", \"review_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95dea96b-73fc-4a54-9714-35d87c2094f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# peek remaining features\n",
    "\n",
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2c9298b-6105-4583-8cd7-102645adc590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed34d9e-f1d6-4d48-a627-fb49e1973357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "hf_location = \"FacebookAI/roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(hf_location, num_labels=condition_labels.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa368b6a-04c3-4541-b9b6-b0232a94ab1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this cell can be skipped - purely explanatory\n",
    "\n",
    "# we make use of the AdamW optimizer with a learning rate of 5e-5 and a linear scheduler\n",
    "# we make this cell explicit here, but AdamW and a scheduled linear learning rate are implied for the \n",
    "# HF Trainer API we eventually use\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b0e31bb-7a9b-4f91-8d69-b7e9957a4b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# move model to GPU\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72930516-0769-421c-a3ab-5422bba3e3d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to keep track of our memory usage as we train\n",
    "\n",
    "def mem_status(): \n",
    "    if torch.cuda.is_available():\n",
    "        gpus = torch.cuda.device_count()\n",
    "        print(\"Memory status: \")\n",
    "        for i in range(gpus):\n",
    "            properties = torch.cuda.get_device_properties(i)\n",
    "            total_memory = properties.total_memory / (1024 ** 3)  # Convert to GB\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / (1024 ** 3)  # Convert to GB\n",
    "            reserved_memory = torch.cuda.memory_reserved(i) / (1024 ** 3)  # Convert to GB\n",
    "            available_memory = total_memory - reserved_memory\n",
    "            print(f\"GPU {i}:\")\n",
    "            print(f\"  Total memory: {total_memory:.2f} GB\")\n",
    "            print(f\"  Allocated memory: {allocated_memory:.2f} GB\")\n",
    "            print(f\"  Reserved memory: {reserved_memory:.2f} GB\")\n",
    "            print(f\"  Available memory: {available_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "mem_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f522c2-e84e-46be-8d99-629112c150c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training loop where we make use of teh HF Trainer for easy setup\n",
    "# We make use of the default AdamW optimizer and linear LR scheduler\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "output_dir = \"/Volumes/workspace_dogfood/jgr/distributed_training_cache/test\" # Author's local save location\n",
    "\n",
    "def train_model():\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "      output_dir=output_dir,\n",
    "      learning_rate=5e-5,\n",
    "      per_device_train_batch_size=16,\n",
    "      per_device_eval_batch_size=16,\n",
    "      num_train_epochs=3,\n",
    "      weight_decay=0.01,\n",
    "      save_strategy=\"epoch\",\n",
    "      report_to=[], # in case we want to enable MLFlow logging\n",
    "      push_to_hub=False,  # we do this manually in the end\n",
    "      load_best_model_at_end=True,\n",
    "      metric_for_best_model=\"eval_loss\",\n",
    "      evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "      model=model,\n",
    "      args=training_args,\n",
    "      train_dataset=tokenized_datasets[\"train\"],\n",
    "      eval_dataset=tokenized_datasets[\"validation\"],\n",
    "      tokenizer=tokenizer,\n",
    "      data_collator=data_collator,\n",
    "    #   compute_metrics=compute_metrics, # we explicitly run our evaluation loop later to account for bugs in the Hugging Face Evaluate library\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer.state.best_model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213e672d-be2e-4b8d-9b9b-f0c465374233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use the PyTorch `TorchDistributor` class from PySpark to run our training on a single-node multi GPU compute instance\n",
    "\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "NUM_PROCESSES = torch.cuda.device_count()\n",
    "print(f\"We're using {NUM_PROCESSES} GPUs\")\n",
    "trained_checkpoint = TorchDistributor(num_processes=NUM_PROCESSES, local_mode=True, use_gpu=True).run(train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ac7c097-bc75-45b1-96ed-01eef3dc187e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate our Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0c8ebf2-cde7-46ea-ae35-4f8dc7f45b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now empty GPU cache before loading model back onto GPUs for evaluation\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb460d94-c2f7-43e2-aa59-a7fdb0efebd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reload our model explicitly into memory to check it out and then evaluate\n",
    "\n",
    "trained_model = AutoModelForSequenceClassification.from_pretrained(trained_checkpoint, device_map=\"auto\", num_labels=condition_labels.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17bbf691-d35c-440d-b6da-71d2893bd4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# look at model details, specifically the number of out_features (805)\n",
    "# this is the number of labels we are predicting and is nontrially 805 since we have dispatched the \n",
    "# old RoBERTa fill-mask model head in favor of a classification head\n",
    "\n",
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab3baaf-98d1-4e34-8282-5b10cea76bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# created a PyTorch validaiton DataLoader for eval\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "validation_dataloader = DataLoader(tokenized_datasets[\"validation\"], shuffle=False, batch_size=8, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "680948fe-8b4a-4e8e-8cb2-04721881880e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hack for testing a single batch through the model\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "    break\n",
    "batch = {k: v.to(torch.device(\"cuda\")) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89a48b73-a08e-4c23-a7de-c1370293470c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# confirm we get logits\n",
    "\n",
    "output = trained_model(**batch)\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08f214bf-8ebf-4901-96af-22ffada2ada4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RELEVANT TO NEXT 3 CELLS:\n",
    "\n",
    "# copy and paste custom evaluation metrics from Author's GitHub for robust multilabel classification\n",
    "# code available here: https://github.com/johngrahamreynolds/FixedMetricsForHF\n",
    "\n",
    "# ideally one could clone this repo or I could create a python small wheel that contains the relevant\n",
    "# classes for download and import it, but C&P is easy enough despite some minimal redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b997948b-d766-444b-9600-e709dcd60cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "from evaluate import evaluator, Metric\n",
    "# from evaluate.metrics.f1 import F1\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# could in principle subclass F1, but ideally we can work the fix into the F1 class to maintain SOLID code\n",
    "class FixedF1(evaluate.Metric):\n",
    "\n",
    "    def __init__(self, average=\"binary\"):\n",
    "        super().__init__()\n",
    "        self.average = average\n",
    "        # additional values passed to compute() could and probably should (?) all be passed here so that the final computation is configured immediately at Metric instantiation\n",
    "\n",
    "    def _info(self):\n",
    "        return evaluate.MetricInfo(\n",
    "            description=\"Custom built F1 metric for true *multilabel* classification - the 'multilabel' config_name var in the evaluate.EvaluationModules class appears to better address multi-class classification, where features can fall under a multitude of labels. Granted, the subtely is minimal and easily confused. This class is implemented with the intention of enabling the evaluation of multiple multilabel classification metrics at the same time using the evaluate.CombinedEvaluations.combine method.\",\n",
    "            citation=\"\",\n",
    "            inputs_description=\"'average': This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Options include: {‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None.\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                    \"references\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                }\n",
    "                if self.config_name == \"multilabel\"\n",
    "                else {\n",
    "                    \"predictions\": datasets.Value(\"int32\"),\n",
    "                    \"references\": datasets.Value(\"int32\"),\n",
    "                }\n",
    "            ),\n",
    "            reference_urls=[\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\"],\n",
    "        )\n",
    "    \n",
    "    # could remove specific kwargs like average, sample_weight from _compute() method of F1\n",
    "\n",
    "    def _compute(self, predictions, references, labels=None, pos_label=1, average=\"binary\", sample_weight=None):\n",
    "        score = f1_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=self.average, sample_weight=sample_weight\n",
    "        )\n",
    "        return {\"f1\": float(score) if score.size == 1 else score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e515016f-b469-43ec-a921-40db6cf48871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "from evaluate import evaluator, Metric\n",
    "# from evaluate.metrics.precision import Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# could in principle subclass Precision, but ideally we can work the fix into the Precision class to maintain SOLID code\n",
    "class FixedPrecision(evaluate.Metric):\n",
    "\n",
    "    def __init__(self, average=\"binary\", zero_division=\"warn\"):\n",
    "        super().__init__()\n",
    "        self.average = average\n",
    "        self.zero_division = zero_division\n",
    "        # additional values passed to compute() could and probably should (?) all be passed here so that the final computation is configured immediately at Metric instantiation\n",
    "\n",
    "    def _info(self):\n",
    "        return evaluate.MetricInfo(\n",
    "            description=\"Custom built Precision metric for true *multilabel* classification - the 'multilabel' config_name var in the evaluate.EvaluationModules class appears to better address multi-class classification, where features can fall under a multitude of labels. Granted, the subtlety is minimal and easily confused. This class is implemented with the intention of enabling the evaluation of multiple multilabel classification metrics at the same time using the evaluate.CombinedEvaluations.combine method.\",\n",
    "            citation=\"\",\n",
    "            inputs_description=\"'average': This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Options include: {‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None.\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                    \"references\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                }\n",
    "                if self.config_name == \"multilabel\"\n",
    "                else {\n",
    "                    \"predictions\": datasets.Value(\"int32\"),\n",
    "                    \"references\": datasets.Value(\"int32\"),\n",
    "                }\n",
    "            ),\n",
    "            reference_urls=[\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\"],\n",
    "        )\n",
    "    \n",
    "    # could remove specific kwargs like average, sample_weight from _compute() method and simply pass them to the underlying scikit-learn function in the form of a class var self.*\n",
    "\n",
    "    def _compute(\n",
    "        self, predictions, references, labels=None, pos_label=1, average=\"binary\", sample_weight=None, zero_division=\"warn\",\n",
    "    ):\n",
    "        score = precision_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=self.average, sample_weight=sample_weight, zero_division=self.zero_division,\n",
    "        )\n",
    "        return {\"precision\": float(score) if score.size == 1 else score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdacbf6f-0231-48ae-b300-f04f15d39b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "from evaluate import evaluator, Metric\n",
    "# from evaluate.metrics.recall import Recall\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# could in principle subclass Recall, but ideally we can work the fix into the Recall class to maintain SOLID code\n",
    "class FixedRecall(evaluate.Metric):\n",
    "\n",
    "    def __init__(self, average=\"binary\"):\n",
    "        super().__init__()\n",
    "        self.average = average\n",
    "        # additional values passed to compute() could and probably should (?) all be passed here so that the final computation is configured immediately at Metric instantiation\n",
    "\n",
    "    def _info(self):\n",
    "        return evaluate.MetricInfo(\n",
    "            description=\"Custom built Recall metric for true *multilabel* classification - the 'multilabel' config_name var in the evaluate.EvaluationModules class appears to better address multi-class classification, where features can fall under a multitude of labels. Granted, the subtlety is minimal and easily confused. This class is implemented with the intention of enabling the evaluation of multiple multilabel classification metrics at the same time using the evaluate.CombinedEvaluations.combine method.\",\n",
    "            citation=\"\",\n",
    "            inputs_description=\"'average': This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Options include: {‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None.\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                    \"references\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                }\n",
    "                if self.config_name == \"multilabel\"\n",
    "                else {\n",
    "                    \"predictions\": datasets.Value(\"int32\"),\n",
    "                    \"references\": datasets.Value(\"int32\"),\n",
    "                }\n",
    "            ),\n",
    "            reference_urls=[\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\"],\n",
    "        )\n",
    "    \n",
    "    # could remove specific kwargs like average, sample_weight from _compute() method and simply pass them to the underlying scikit-learn function in the form of a class var self.*\n",
    "\n",
    "    def _compute(\n",
    "        self, predictions, references, labels=None, pos_label=1, average=\"binary\", sample_weight=None, zero_division=\"warn\",\n",
    "    ):\n",
    "        score = recall_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=self.average, sample_weight=sample_weight, zero_division=zero_division,\n",
    "        )\n",
    "        return {\"recall\": float(score) if score.size == 1 else score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03367a0b-c9bb-4df4-9d1b-5814ccbe9de7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# instaniate our combined custom metrics\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "f1 = FixedF1(average=\"weighted\")\n",
    "precision = FixedPrecision(average=\"weighted\", zero_division=np.nan)\n",
    "recall = FixedRecall(average=\"weighted\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "combined = evaluate.combine([f1, accuracy, recall, precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a49ac47d-b2e7-40ce-99cc-5d78fcd9ddf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we run our evaluation loop\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "num_batches = len(test_dataloader) \n",
    "progress_bar = tqdm(range(num_batches))\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "# evaluation loop\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(torch.device(\"cuda\")) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    combined.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "combined.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6b80301-6fb8-478e-9326-250e50f7d962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Push our Model to the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2fd85c7-5ae9-4a9c-af6a-85a65a850d2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# the model is pushed to the hub with the standard\n",
    "model_name = \"RoBERTa-base-DReiFT\"\n",
    "trained_model.push_to_hub(model_name, commit_message=\"Pushing fine-tuned RoBERTa model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "524f8924-f0e5-43a0-a727-4335afb57460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lastly, we make one final modification to the model's config by updating the label names to\n",
    "# reflect the medical conditions that we are classifying\n",
    "# This will improve ease of inference\n",
    "\n",
    "id2label_dict = dict()\n",
    "for i in range(condition_labels.num_classes):\n",
    "    id2label_dict[i] = condition_labels.int2str(i)\n",
    "label2id_dict = {v: k for k, v in id2label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "850eb261-1207-4adb-9fdd-c10779c3fd76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we reload the model from the hub but correct the config as we want it with the proper label dicts\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "# define config\n",
    "config = AutoConfig.from_pretrained(model_name, label2id=label2id_dict, id2label=id2label_dict)\n",
    "\n",
    "# load model with config\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "# repush corrected model\n",
    "model.push_to_hub(model_name, commit_message=\"Repushing corrected RoBERTa model with proper label names in config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8813bf5-660a-4397-a58c-2908e7a50ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d354921-59ee-4627-aacc-c29bc528ac2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now, as detailed in the model card, one can use the model as follows:\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"MarioBarbeque/RoBERTa-base-DReiFT\"\n",
    "tokenizer_name = \"FacebookAI/roberta-base\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# Pass a unique 'drug-review' to classify the underlying condition based upon 805 pretrained medical issues\n",
    "drug_review = [\"My tonsils were swollen and I had a hard time swallowing. I had a minimal fever to accompany the pain in my throat. Taking Aleve at regular intervals throughout the day improved my swallowing. I am now taking Aleve every 4 hours.\"]\n",
    "tokenized_review = tokenizer(drug_review, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
    "\n",
    "output = model(**tokenized_review)\n",
    "predicted_label = model.config.id2label[torch.argmax(output.logits, dim=-1).item()]\n",
    "print(f\"The model predicted the underlying condition to be: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Training and Evaluation: RoBERTa-base-DReiFT",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
